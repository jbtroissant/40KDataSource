import requests
import os
import json
from urllib.parse import urljoin

def download_json_files():
    """
    T√©l√©charge une liste de fichiers JSON depuis l'URL GitHub sp√©cifi√©e.
    """
    # URL de base
    base_url = "https://raw.githubusercontent.com/game-datacards/datasources/main/10th/gdc/"
    
    # Liste des fichiers JSON √† t√©l√©charger
    json_files = [
        "adeptasororitas.json",
        "adeptuscustodes.json",
        "adeptusmechanicus.json",
        "aeldari.json",
        "agents.json",
        "astramilitarum.json",
        "blacktemplar.json",
        "bloodangels.json",
        "chaos_spacemarines.json",
        "chaosdaemons.json",
        "chaosknights.json",
        "darkangels.json",
        "deathguard.json",
        "deathwatch.json",
        "drukhari.json",
        "emperors_children.json",
        "greyknights.json",
        "gsc.json",
        "imperialknights.json",
        "necrons.json",
        "orks.json",
        "space_marines.json",
        "spacewolves.json",
        "tau.json",
        "thousandsons.json",
        "tyranids.json",
        "unaligned.json",
        "votann.json",
        "worldeaters.json"
    ]
    
    # Utiliser le dossier archive comme destination
    output_dir = "archive"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Dossier '{output_dir}' cr√©√©.")
    
    # T√©l√©charger chaque fichier
    for filename in json_files:
        file_url = urljoin(base_url, filename)
        output_path = os.path.join(output_dir, filename)
        
        try:
            print(f"T√©l√©chargement de {filename}...")
            response = requests.get(file_url)
            response.raise_for_status()  # L√®ve une exception si la requ√™te √©choue
            
            # Sauvegarder le fichier
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            print(f"‚úì {filename} t√©l√©charg√© avec succ√®s")
            
        except requests.exceptions.RequestException as e:
            print(f"‚úó Erreur lors du t√©l√©chargement de {filename}: {e}")
        except Exception as e:
            print(f"‚úó Erreur lors de la sauvegarde de {filename}: {e}")
    
    print(f"\nT√©l√©chargement termin√©. Les fichiers sont dans le dossier '{output_dir}' et ont remplac√© les fichiers existants.")
    
    # Nettoyer space_marines.json en supprimant les datasheets en double
    clean_space_marines_json()

def clean_space_marines_json():
    """
    Supprime les datasheets de space_marines.json qui existent d√©j√† dans les autres fichiers de chapitres.
    """
    print("\nüßπ Nettoyage de space_marines.json...")
    
    # Fichiers contenant des datasheets √† exclure
    chapter_files = [
        "spacewolves.json",
        "agents.json", 
        "bloodangels.json",
        "blacktemplar.json",
        "darkangels.json",
        "deathwatch.json"
    ]
    
    # Charger space_marines.json
    space_marines_path = os.path.join("archive", "space_marines.json")
    if not os.path.exists(space_marines_path):
        print("‚ùå space_marines.json non trouv√©")
        return
    
    try:
        with open(space_marines_path, 'r', encoding='utf-8') as f:
            space_marines_data = json.load(f)
        
        # Collecter tous les noms de datasheets des chapitres
        chapter_datasheet_names = set()
        
        for chapter_file in chapter_files:
            chapter_path = os.path.join("archive", chapter_file)
            if os.path.exists(chapter_path):
                try:
                    with open(chapter_path, 'r', encoding='utf-8') as f:
                        chapter_data = json.load(f)
                    
                    if 'datasheets' in chapter_data:
                        for datasheet in chapter_data['datasheets']:
                            if 'name' in datasheet:
                                chapter_datasheet_names.add(datasheet['name'])
                    
                    print(f"üìñ {chapter_file}: {len(chapter_data.get('datasheets', []))} datasheets lues")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur lors de la lecture de {chapter_file}: {e}")
            else:
                print(f"‚ö†Ô∏è {chapter_file} non trouv√©")
        
        print(f"üìã Total des datasheets des chapitres: {len(chapter_datasheet_names)}")
        
        # Filtrer les datasheets de space_marines.json
        original_count = len(space_marines_data.get('datasheets', []))
        filtered_datasheets = []
        removed_count = 0
        
        for datasheet in space_marines_data.get('datasheets', []):
            if 'name' in datasheet:
                if datasheet['name'] in chapter_datasheet_names:
                    print(f"üóëÔ∏è Suppression: {datasheet['name']}")
                    removed_count += 1
                else:
                    filtered_datasheets.append(datasheet)
            else:
                # Garder les datasheets sans nom
                filtered_datasheets.append(datasheet)
        
        # Mettre √† jour space_marines.json
        space_marines_data['datasheets'] = filtered_datasheets
        
        # Sauvegarder avec backup
        backup_path = space_marines_path + ".backup"
        if not os.path.exists(backup_path):
            os.rename(space_marines_path, backup_path)
            print(f"üíæ Backup cr√©√©: {backup_path}")
        
        with open(space_marines_path, 'w', encoding='utf-8') as f:
            json.dump(space_marines_data, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Nettoyage termin√©:")
        print(f"   - Datasheets originales: {original_count}")
        print(f"   - Datasheets supprim√©es: {removed_count}")
        print(f"   - Datasheets restantes: {len(filtered_datasheets)}")
        
    except Exception as e:
        print(f"‚ùå Erreur lors du nettoyage: {e}")

if __name__ == "__main__":
    download_json_files() 